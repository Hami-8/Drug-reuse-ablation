{"sample_index": 2, "subset": "indication", "disease": "Atypical haemolytic uraemic syndrome", "relation": "indication", "gold_drugs": ["eculizumab"], "seeds": ["eculizumab", "ravulizumab"], "predictions": [], "ranking": ["eculizumab", "ravulizumab"], "top_recommendations": ["eculizumab", "ravulizumab"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 2, "vote_table": [{"canonical": "eculizumab", "votes": 15, "avg_rank": 1.0, "rep": "eculizumab"}, {"canonical": "ravulizumab", "votes": 15, "avg_rank": 2.0, "rep": "ravulizumab"}]}, "note": "ok"}
{"sample_index": 5, "subset": "indication", "disease": "Generalised pustular psoriasis", "relation": "indication", "gold_drugs": ["certolizumab pegol", "spesolimab"], "seeds": ["acitretin", "cyclosporine", "methotrexate", "infliximab", "adalimumab", "etanercept", "secukinumab", "guselkumab", "ustekinumab", "ixekizumab", "brodalumab", "risankizumab"], "predictions": [], "ranking": ["acitretin", "cyclosporine", "methotrexate", "infliximab", "adalimumab", "etanercept", "secukinumab", "guselkumab", "ustekinumab", "ixekizumab", "brodalumab", "risankizumab"], "top_recommendations": ["acitretin", "cyclosporine", "methotrexate"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 25, "vote_table": [{"canonical": "acitretin", "votes": 14, "avg_rank": 1.0, "rep": "acitretin"}, {"canonical": "cyclosporine", "votes": 14, "avg_rank": 2.5, "rep": "cyclosporine"}, {"canonical": "methotrexate", "votes": 14, "avg_rank": 2.5, "rep": "methotrexate"}, {"canonical": "infliximab", "votes": 14, "avg_rank": 4.0, "rep": "infliximab"}, {"canonical": "adalimumab", "votes": 14, "avg_rank": 5.0714, "rep": "adalimumab"}, {"canonical": "etanercept", "votes": 14, "avg_rank": 5.9286, "rep": "etanercept"}, {"canonical": "secukinumab", "votes": 14, "avg_rank": 7.3571, "rep": "secukinumab"}, {"canonical": "guselkumab", "votes": 14, "avg_rank": 10.0, "rep": "guselkumab"}, {"canonical": "ustekinumab", "votes": 13, "avg_rank": 7.8462, "rep": "ustekinumab"}, {"canonical": "ixekizumab", "votes": 11, "avg_rank": 8.9091, "rep": "ixekizumab"}, {"canonical": "brodalumab", "votes": 11, "avg_rank": 10.4545, "rep": "brodalumab"}, {"canonical": "risankizumab", "votes": 7, "avg_rank": 11.0, "rep": "risankizumab"}, {"canonical": "certolizumab pegol", "votes": 6, "avg_rank": 11.6667, "rep": "Certolizumab pegol"}, {"canonical": "apremilast", "votes": 3, "avg_rank": 11.3333, "rep": "Apremilast"}, {"canonical": "spesolimab", "votes": 3, "avg_rank": 11.6667, "rep": "Spesolimab"}, {"canonical": "tildrakizumab", "votes": 2, "avg_rank": 12.0, "rep": "tildrakizumab"}, {"canonical": "{\"answers\" \"acitretin\"", "votes": 1, "avg_rank": 1.0, "rep": "{\"answers\": [\"Acitretin\""}, {"canonical": "\"methotrexate\"", "votes": 1, "avg_rank": 2.0, "rep": "\"Methotrexate\""}, {"canonical": "\"cyclosporine\"", "votes": 1, "avg_rank": 3.0, "rep": "\"Cyclosporine\""}, {"canonical": "\"infliximab\"", "votes": 1, "avg_rank": 4.0, "rep": "\"Infliximab\""}, {"canonical": "\"adalimumab\"", "votes": 1, "avg_rank": 5.0, "rep": "\"Adalimumab\""}, {"canonical": "\"etanercept\"", "votes": 1, "avg_rank": 6.0, "rep": "\"Etanercept\""}, {"canonical": "\"secukinumab\"", "votes": 1, "avg_rank": 7.0, "rep": "\"Secukinumab\""}, {"canonical": "\"brodalumab\"", "votes": 1, "avg_rank": 8.0, "rep": "\"Brodalumab\""}, {"canonical": "\"ixekizumab\"", "votes": 1, "avg_rank": 9.0, "rep": "\"Ixekizumab\""}]}, "note": "ok"}
{"sample_index": 4, "subset": "indication", "disease": "Gastrointestinal epithelial barrier disorders", "relation": "indication", "gold_drugs": ["teduglutide"], "seeds": ["mesalamine", "budesonide", "infliximab", "azathioprine", "adalimumab", "vedolizumab", "prednisone", "ustekinumab", "sulfasalazine", "certolizumab pegol", "tofacitinib", "6-mercaptopurine"], "predictions": [], "ranking": ["mesalamine", "budesonide", "infliximab", "azathioprine", "adalimumab", "vedolizumab", "prednisone", "ustekinumab", "sulfasalazine", "certolizumab pegol", "tofacitinib", "6-mercaptopurine"], "top_recommendations": ["mesalamine", "budesonide", "infliximab"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 21, "vote_table": [{"canonical": "mesalamine", "votes": 15, "avg_rank": 1.0, "rep": "mesalamine"}, {"canonical": "budesonide", "votes": 15, "avg_rank": 2.6667, "rep": "budesonide"}, {"canonical": "infliximab", "votes": 15, "avg_rank": 5.7333, "rep": "infliximab"}, {"canonical": "azathioprine", "votes": 15, "avg_rank": 5.8667, "rep": "azathioprine"}, {"canonical": "adalimumab", "votes": 15, "avg_rank": 6.7333, "rep": "adalimumab"}, {"canonical": "vedolizumab", "votes": 15, "avg_rank": 8.1333, "rep": "vedolizumab"}, {"canonical": "prednisone", "votes": 14, "avg_rank": 3.5, "rep": "prednisone"}, {"canonical": "ustekinumab", "votes": 14, "avg_rank": 9.9286, "rep": "ustekinumab"}, {"canonical": "sulfasalazine", "votes": 9, "avg_rank": 2.0, "rep": "sulfasalazine"}, {"canonical": "certolizumab pegol", "votes": 9, "avg_rank": 10.5556, "rep": "certolizumab pegol"}, {"canonical": "tofacitinib", "votes": 8, "avg_rank": 10.875, "rep": "tofacitinib"}, {"canonical": "6 mercaptopurine", "votes": 7, "avg_rank": 6.5714, "rep": "6-mercaptopurine"}, {"canonical": "methotrexate", "votes": 7, "avg_rank": 10.5714, "rep": "methotrexate"}, {"canonical": "golimumab", "votes": 7, "avg_rank": 11.1429, "rep": "golimumab"}, {"canonical": "mercaptopurine", "votes": 6, "avg_rank": 7.6667, "rep": "mercaptopurine"}, {"canonical": "certolizumab", "votes": 3, "avg_rank": 9.0, "rep": "certolizumab"}, {"canonical": "rifaximin", "votes": 2, "avg_rank": 11.0, "rep": "rifaximin"}, {"canonical": "prednisolone", "votes": 1, "avg_rank": 4.0, "rep": "prednisolone"}, {"canonical": "cyclosporine", "votes": 1, "avg_rank": 10.0, "rep": "cyclosporine"}, {"canonical": "omeprazole", "votes": 1, "avg_rank": 11.0, "rep": "omeprazole"}, {"canonical": "esomeprazole", "votes": 1, "avg_rank": 12.0, "rep": "esomeprazole"}]}, "note": "ok"}
{"sample_index": 3, "subset": "indication", "disease": "Epileptic encephalopathy", "relation": "indication", "gold_drugs": ["anti-seizure medication", "matching placebo", "tak-935", "zx008"], "seeds": ["valproate", "levetiracetam", "clobazam", "topiramate", "rufinamide", "phenobarbital", "lamotrigine", "stiripentol", "cannabidiol", "vigabatrin", "zonisamide", "clonazepam"], "predictions": [], "ranking": ["valproate", "levetiracetam", "clobazam", "topiramate", "rufinamide", "phenobarbital", "lamotrigine", "stiripentol", "cannabidiol", "vigabatrin", "zonisamide", "clonazepam"], "top_recommendations": ["valproate", "levetiracetam", "clobazam"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 38, "vote_table": [{"canonical": "valproate", "votes": 12, "avg_rank": 1.0, "rep": "valproate"}, {"canonical": "levetiracetam", "votes": 12, "avg_rank": 2.3333, "rep": "levetiracetam"}, {"canonical": "clobazam", "votes": 12, "avg_rank": 3.9167, "rep": "clobazam"}, {"canonical": "topiramate", "votes": 12, "avg_rank": 4.6667, "rep": "topiramate"}, {"canonical": "rufinamide", "votes": 12, "avg_rank": 6.25, "rep": "rufinamide"}, {"canonical": "phenobarbital", "votes": 12, "avg_rank": 9.3333, "rep": "phenobarbital"}, {"canonical": "lamotrigine", "votes": 11, "avg_rank": 5.3636, "rep": "lamotrigine"}, {"canonical": "stiripentol", "votes": 11, "avg_rank": 6.2727, "rep": "stiripentol"}, {"canonical": "cannabidiol", "votes": 11, "avg_rank": 8.4545, "rep": "cannabidiol"}, {"canonical": "vigabatrin", "votes": 9, "avg_rank": 8.7778, "rep": "vigabatrin"}, {"canonical": "zonisamide", "votes": 9, "avg_rank": 10.2222, "rep": "zonisamide"}, {"canonical": "clonazepam", "votes": 6, "avg_rank": 8.8333, "rep": "clonazepam"}, {"canonical": "lacosamide", "votes": 5, "avg_rank": 10.0, "rep": "lacosamide"}, {"canonical": "felbamate", "votes": 4, "avg_rank": 10.25, "rep": "felbamate"}, {"canonical": "error=none", "votes": 2, "avg_rank": 3.0, "rep": "error=None"}, {"canonical": "incomplete details=incompletedetails reason='max output tokens'", "votes": 2, "avg_rank": 4.0, "rep": "incomplete_details=IncompleteDetails(reason='max_output_tokens')"}, {"canonical": "instructions=none", "votes": 2, "avg_rank": 5.0, "rep": "instructions=None"}, {"canonical": "metadata={}", "votes": 2, "avg_rank": 6.0, "rep": "metadata={}"}, {"canonical": "model='o3 mini 2025 01 31'", "votes": 2, "avg_rank": 7.0, "rep": "model='o3-mini-2025-01-31'"}, {"canonical": "object='response'", "votes": 2, "avg_rank": 8.0, "rep": "object='response'"}, {"canonical": "summary=", "votes": 2, "avg_rank": 10.0, "rep": "summary=[]"}, {"canonical": "type='reasoning'", "votes": 2, "avg_rank": 11.0, "rep": "type='reasoning'"}, {"canonical": "brivaracetam", "votes": 2, "avg_rank": 11.5, "rep": "brivaracetam"}, {"canonical": "fenfluramine", "votes": 2, "avg_rank": 12.0, "rep": "Fenfluramine"}, {"canonical": "content=none", "votes": 2, "avg_rank": 12.0, "rep": "content=None"}, {"canonical": "{\"answers\" \"valproate\"", "votes": 1, "avg_rank": 1.0, "rep": "{\"answers\": [\"valproate\""}, {"canonical": "response id='resp 006b9bc3fea50c210068e1e9e31b2c8195a5e7c0fe56143351'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_006b9bc3fea50c210068e1e9e31b2c8195a5e7c0fe56143351'"}, {"canonical": "response id='resp 05e15d6fa6d119c60068e1e9b3d3e88195b292b51234b5ccf3'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_05e15d6fa6d119c60068e1e9b3d3e88195b292b51234b5ccf3'"}, {"canonical": "\"levetiracetam\"", "votes": 1, "avg_rank": 2.0, "rep": "\"levetiracetam\""}, {"canonical": "created at=1759635891.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635891.0"}, {"canonical": "created at=1759635939.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635939.0"}, {"canonical": "\"topiramate\"", "votes": 1, "avg_rank": 3.0, "rep": "\"topiramate\""}, {"canonical": "\"rufinamide\"", "votes": 1, "avg_rank": 4.0, "rep": "\"rufinamide\""}, {"canonical": "\"clobazam", "votes": 1, "avg_rank": 5.0, "rep": "\"clobazam"}, {"canonical": "output= responsereasoningitem id='rs 006b9bc3fea50c210068e1e9eb74248195a07b11dc2dd59e20'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_006b9bc3fea50c210068e1e9eb74248195a07b11dc2dd59e20'"}, {"canonical": "output= responsereasoningitem id='rs 05e15d6fa6d119c60068e1e9bc496c8195a84af86d4116f0fe'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_05e15d6fa6d119c60068e1e9bc496c8195a84af86d4116f0fe'"}, {"canonical": "oxcarbazepine", "votes": 1, "avg_rank": 11.0, "rep": "oxcarbazepine"}, {"canonical": "adrenocorticotropic hormone", "votes": 1, "avg_rank": 12.0, "rep": "adrenocorticotropic hormone"}]}, "note": "ok"}
{"sample_index": 0, "subset": "indication", "disease": "Amyotrophic lateral sclerosis or motor neuron disease", "relation": "indication", "gold_drugs": ["dextromethorphan + quinidine", "escitalopram"], "seeds": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None", "metadata={}", "model='o3-mini-2025-01-31'", "object='response'", "summary=[]", "type='reasoning'", "content=None", "riluzole", "edaravone", "Response(id='resp_0312b8115685166c0068e1e9d617448197a8f8913764981f9c'"], "predictions": [], "ranking": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None", "metadata={}", "model='o3-mini-2025-01-31'", "object='response'", "summary=[]", "type='reasoning'", "content=None", "riluzole", "edaravone", "Response(id='resp_0312b8115685166c0068e1e9d617448197a8f8913764981f9c'"], "top_recommendations": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 45, "vote_table": [{"canonical": "error=none", "votes": 11, "avg_rank": 3.0, "rep": "error=None"}, {"canonical": "incomplete details=incompletedetails reason='max output tokens'", "votes": 11, "avg_rank": 4.0, "rep": "incomplete_details=IncompleteDetails(reason='max_output_tokens')"}, {"canonical": "instructions=none", "votes": 11, "avg_rank": 5.0, "rep": "instructions=None"}, {"canonical": "metadata={}", "votes": 11, "avg_rank": 6.0, "rep": "metadata={}"}, {"canonical": "model='o3 mini 2025 01 31'", "votes": 11, "avg_rank": 7.0, "rep": "model='o3-mini-2025-01-31'"}, {"canonical": "object='response'", "votes": 11, "avg_rank": 8.0, "rep": "object='response'"}, {"canonical": "summary=", "votes": 11, "avg_rank": 10.0, "rep": "summary=[]"}, {"canonical": "type='reasoning'", "votes": 11, "avg_rank": 11.0, "rep": "type='reasoning'"}, {"canonical": "content=none", "votes": 11, "avg_rank": 12.0, "rep": "content=None"}, {"canonical": "riluzole", "votes": 4, "avg_rank": 1.0, "rep": "riluzole"}, {"canonical": "edaravone", "votes": 4, "avg_rank": 2.0, "rep": "edaravone"}, {"canonical": "response id='resp 0312b8115685166c0068e1e9d617448197a8f8913764981f9c'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0312b8115685166c0068e1e9d617448197a8f8913764981f9c'"}, {"canonical": "response id='resp 0438c410dbfabe680068e1ea02d0c881938312f09c6d422034'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0438c410dbfabe680068e1ea02d0c881938312f09c6d422034'"}, {"canonical": "response id='resp 0456a9f56f54529f0068e1e9b586b88197a49e79d64183fa4e'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0456a9f56f54529f0068e1e9b586b88197a49e79d64183fa4e'"}, {"canonical": "response id='resp 058db9ea1977d5310068e1e980a4308196bb34872c2b6c4772'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_058db9ea1977d5310068e1e980a4308196bb34872c2b6c4772'"}, {"canonical": "response id='resp 06753150afaa2f8e0068e1e98da83c81969f07ed58b86aa365'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_06753150afaa2f8e0068e1e98da83c81969f07ed58b86aa365'"}, {"canonical": "response id='resp 075308f49d8973510068e1e9ec496c819599169d3149fdcb93'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_075308f49d8973510068e1e9ec496c819599169d3149fdcb93'"}, {"canonical": "response id='resp 0a7e1950dbb134420068e1e9cb969081909d526c6e4d500498'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0a7e1950dbb134420068e1e9cb969081909d526c6e4d500498'"}, {"canonical": "response id='resp 0bcb2036fd5463060068e1e9f7f22881948a2b6439724bda9f'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0bcb2036fd5463060068e1e9f7f22881948a2b6439724bda9f'"}, {"canonical": "response id='resp 0c94860bea9933750068e1e9a3c4b4819385b5a234f54f917f'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0c94860bea9933750068e1e9a3c4b4819385b5a234f54f917f'"}, {"canonical": "response id='resp 0cc8d3ad2a01b5330068e1e9bef5ac81949778748d42986471'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0cc8d3ad2a01b5330068e1e9bef5ac81949778748d42986471'"}, {"canonical": "response id='resp 0f4bfb207ea1f8ef0068e1e9dff6848196b96c14797515050e'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0f4bfb207ea1f8ef0068e1e9dff6848196b96c14797515050e'"}, {"canonical": "created at=1759635840.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635840.0"}, {"canonical": "created at=1759635853.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635853.0"}, {"canonical": "created at=1759635875.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635875.0"}, {"canonical": "created at=1759635893.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635893.0"}, {"canonical": "created at=1759635903.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635903.0"}, {"canonical": "created at=1759635915.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635915.0"}, {"canonical": "created at=1759635926.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635926.0"}, {"canonical": "created at=1759635936.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635936.0"}, {"canonical": "created at=1759635948.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635948.0"}, {"canonical": "created at=1759635960.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635960.0"}, {"canonical": "created at=1759635970.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635970.0"}, {"canonical": "phenylbutyrate taurursodiol", "votes": 1, "avg_rank": 3.0, "rep": "sodium phenylbutyrate/taurursodiol"}, {"canonical": "output= responsereasoningitem id='rs 0312b8115685166c0068e1e9dd161c8197a0da7f1c4c4c87b4'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0312b8115685166c0068e1e9dd161c8197a0da7f1c4c4c87b4'"}, {"canonical": "output= responsereasoningitem id='rs 0438c410dbfabe680068e1ea0b8c388193bcf8a85a68284d63'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0438c410dbfabe680068e1ea0b8c388193bcf8a85a68284d63'"}, {"canonical": "output= responsereasoningitem id='rs 0456a9f56f54529f0068e1e9bcaa488197b8cbd5f26b649d7e'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0456a9f56f54529f0068e1e9bcaa488197b8cbd5f26b649d7e'"}, {"canonical": "output= responsereasoningitem id='rs 058db9ea1977d5310068e1e987ad8c8196879c6d78dddf73ca'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_058db9ea1977d5310068e1e987ad8c8196879c6d78dddf73ca'"}, {"canonical": "output= responsereasoningitem id='rs 06753150afaa2f8e0068e1e99432548196b2a398af9cbe19da'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_06753150afaa2f8e0068e1e99432548196b2a398af9cbe19da'"}, {"canonical": "output= responsereasoningitem id='rs 075308f49d8973510068e1e9f5d9148195974dbd0a8dd388b3'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_075308f49d8973510068e1e9f5d9148195974dbd0a8dd388b3'"}, {"canonical": "output= responsereasoningitem id='rs 0a7e1950dbb134420068e1e9d411f48190b0d971ce04c8891f'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0a7e1950dbb134420068e1e9d411f48190b0d971ce04c8891f'"}, {"canonical": "output= responsereasoningitem id='rs 0bcb2036fd5463060068e1e9ff425881949d9d6a40bf736cf8'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0bcb2036fd5463060068e1e9ff425881949d9d6a40bf736cf8'"}, {"canonical": "output= responsereasoningitem id='rs 0c94860bea9933750068e1e9ab813c8193818ad79978d675b9'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0c94860bea9933750068e1e9ab813c8193818ad79978d675b9'"}, {"canonical": "output= responsereasoningitem id='rs 0cc8d3ad2a01b5330068e1e9c64e688194847bab533e11769b'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0cc8d3ad2a01b5330068e1e9c64e688194847bab533e11769b'"}, {"canonical": "output= responsereasoningitem id='rs 0f4bfb207ea1f8ef0068e1e9e7946481969a18d9a8e8ca51a8'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0f4bfb207ea1f8ef0068e1e9e7946481969a18d9a8e8ca51a8'"}]}, "note": "ok"}
{"sample_index": 1, "subset": "indication", "disease": "Arrhythmogenic Right Ventricular Cardiomyopathy", "relation": "indication", "gold_drugs": ["flecainide pill"], "seeds": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None", "metadata={}", "model='o3-mini-2025-01-31'", "object='response'", "summary=[]", "type='reasoning'", "content=None", "Response(id='resp_0072eddd633438ef0068e1e9b3b18081959db6904885d80e18'", "Response(id='resp_014e034f1af254490068e1e9e1f1d08190afaec668f05c8ff3'", "Response(id='resp_016fad799d0e68550068e1e9ecb06c8190a00bd0d24f8c816d'"], "predictions": [], "ranking": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None", "metadata={}", "model='o3-mini-2025-01-31'", "object='response'", "summary=[]", "type='reasoning'", "content=None", "Response(id='resp_0072eddd633438ef0068e1e9b3b18081959db6904885d80e18'", "Response(id='resp_014e034f1af254490068e1e9e1f1d08190afaec668f05c8ff3'", "Response(id='resp_016fad799d0e68550068e1e9ecb06c8190a00bd0d24f8c816d'"], "top_recommendations": ["error=None", "incomplete_details=IncompleteDetails(reason='max_output_tokens')", "instructions=None"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 54, "vote_table": [{"canonical": "error=none", "votes": 15, "avg_rank": 3.0, "rep": "error=None"}, {"canonical": "incomplete details=incompletedetails reason='max output tokens'", "votes": 15, "avg_rank": 4.0, "rep": "incomplete_details=IncompleteDetails(reason='max_output_tokens')"}, {"canonical": "instructions=none", "votes": 15, "avg_rank": 5.0, "rep": "instructions=None"}, {"canonical": "metadata={}", "votes": 15, "avg_rank": 6.0, "rep": "metadata={}"}, {"canonical": "model='o3 mini 2025 01 31'", "votes": 15, "avg_rank": 7.0, "rep": "model='o3-mini-2025-01-31'"}, {"canonical": "object='response'", "votes": 15, "avg_rank": 8.0, "rep": "object='response'"}, {"canonical": "summary=", "votes": 15, "avg_rank": 10.0, "rep": "summary=[]"}, {"canonical": "type='reasoning'", "votes": 15, "avg_rank": 11.0, "rep": "type='reasoning'"}, {"canonical": "content=none", "votes": 15, "avg_rank": 12.0, "rep": "content=None"}, {"canonical": "response id='resp 0072eddd633438ef0068e1e9b3b18081959db6904885d80e18'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0072eddd633438ef0068e1e9b3b18081959db6904885d80e18'"}, {"canonical": "response id='resp 014e034f1af254490068e1e9e1f1d08190afaec668f05c8ff3'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_014e034f1af254490068e1e9e1f1d08190afaec668f05c8ff3'"}, {"canonical": "response id='resp 016fad799d0e68550068e1e9ecb06c8190a00bd0d24f8c816d'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_016fad799d0e68550068e1e9ecb06c8190a00bd0d24f8c816d'"}, {"canonical": "response id='resp 019e6d339fc355a90068e1e9d559948196b8c1ed2e0d0bd421'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_019e6d339fc355a90068e1e9d559948196b8c1ed2e0d0bd421'"}, {"canonical": "response id='resp 0378f9a0b12f20c40068e1e97b9ae08194bcf4c644c8b42c0b'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0378f9a0b12f20c40068e1e97b9ae08194bcf4c644c8b42c0b'"}, {"canonical": "response id='resp 04bffbd15b664b3a0068e1e9c17a4c819482ce9f060b112fab'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_04bffbd15b664b3a0068e1e9c17a4c819482ce9f060b112fab'"}, {"canonical": "response id='resp 05ab9c9b428cf4b60068e1ea19f9888196ac02a94e787d646f'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_05ab9c9b428cf4b60068e1ea19f9888196ac02a94e787d646f'"}, {"canonical": "response id='resp 077ea9b7d5a7daef0068e1e9f8ffac819695992fdce7350121'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_077ea9b7d5a7daef0068e1e9f8ffac819695992fdce7350121'"}, {"canonical": "response id='resp 0857e45152b33e0c0068e1e994b2888197a07a8194e3a9b5ed'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0857e45152b33e0c0068e1e994b2888197a07a8194e3a9b5ed'"}, {"canonical": "response id='resp 0ba81b0896ac123a0068e1e99f1334819587f79a23c37d77d6'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0ba81b0896ac123a0068e1e99f1334819587f79a23c37d77d6'"}, {"canonical": "response id='resp 0bc1f454cf4750db0068e1e9cb65b48196beef955eb58923d5'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0bc1f454cf4750db0068e1e9cb65b48196beef955eb58923d5'"}, {"canonical": "response id='resp 0cb30393f0dd32e30068e1e9a80a3c8195b1df707949cbed0c'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0cb30393f0dd32e30068e1e9a80a3c8195b1df707949cbed0c'"}, {"canonical": "response id='resp 0eb9248367c74bba0068e1e98a74a081949b4ed3032b541955'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0eb9248367c74bba0068e1e98a74a081949b4ed3032b541955'"}, {"canonical": "response id='resp 0f3e17a697e68fb70068e1ea0f6a148194981fab1e721d1c4d'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0f3e17a697e68fb70068e1ea0f6a148194981fab1e721d1c4d'"}, {"canonical": "response id='resp 0fd73a993861f79e0068e1ea06563c8193961415557c9d5150'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_0fd73a993861f79e0068e1ea06563c8193961415557c9d5150'"}, {"canonical": "created at=1759635835.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635835.0"}, {"canonical": "created at=1759635850.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635850.0"}, {"canonical": "created at=1759635860.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635860.0"}, {"canonical": "created at=1759635871.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635871.0"}, {"canonical": "created at=1759635880.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635880.0"}, {"canonical": "created at=1759635891.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635891.0"}, {"canonical": "created at=1759635905.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635905.0"}, {"canonical": "created at=1759635915.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635915.0"}, {"canonical": "created at=1759635925.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635925.0"}, {"canonical": "created at=1759635937.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635937.0"}, {"canonical": "created at=1759635948.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635948.0"}, {"canonical": "created at=1759635961.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635961.0"}, {"canonical": "created at=1759635974.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635974.0"}, {"canonical": "created at=1759635983.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635983.0"}, {"canonical": "created at=1759635994.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635994.0"}, {"canonical": "output= responsereasoningitem id='rs 0072eddd633438ef0068e1e9bb02d881958cd611020ffc60e2'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0072eddd633438ef0068e1e9bb02d881958cd611020ffc60e2'"}, {"canonical": "output= responsereasoningitem id='rs 014e034f1af254490068e1e9e8e030819083e520b3b7a609f2'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_014e034f1af254490068e1e9e8e030819083e520b3b7a609f2'"}, {"canonical": "output= responsereasoningitem id='rs 016fad799d0e68550068e1e9f3ec508190b549695be736754e'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_016fad799d0e68550068e1e9f3ec508190b549695be736754e'"}, {"canonical": "output= responsereasoningitem id='rs 019e6d339fc355a90068e1e9dda934819685e7ab01cbb139c0'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_019e6d339fc355a90068e1e9dda934819685e7ab01cbb139c0'"}, {"canonical": "output= responsereasoningitem id='rs 0378f9a0b12f20c40068e1e98435e08194afa73f890b5ff85d'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0378f9a0b12f20c40068e1e98435e08194afa73f890b5ff85d'"}, {"canonical": "output= responsereasoningitem id='rs 04bffbd15b664b3a0068e1e9c8ec5c8194a3abb992cfe6a238'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_04bffbd15b664b3a0068e1e9c8ec5c8194a3abb992cfe6a238'"}, {"canonical": "output= responsereasoningitem id='rs 05ab9c9b428cf4b60068e1ea20cd548196a3b6783fb17f7e4e'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_05ab9c9b428cf4b60068e1ea20cd548196a3b6783fb17f7e4e'"}, {"canonical": "output= responsereasoningitem id='rs 077ea9b7d5a7daef0068e1ea02755481968ccc699d388f2863'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_077ea9b7d5a7daef0068e1ea02755481968ccc699d388f2863'"}, {"canonical": "output= responsereasoningitem id='rs 0857e45152b33e0c0068e1e99bd9b081979d03b10c6e22ed98'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0857e45152b33e0c0068e1e99bd9b081979d03b10c6e22ed98'"}, {"canonical": "output= responsereasoningitem id='rs 0ba81b0896ac123a0068e1e9a58c408195a8d1b6d530901150'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0ba81b0896ac123a0068e1e9a58c408195a8d1b6d530901150'"}, {"canonical": "output= responsereasoningitem id='rs 0bc1f454cf4750db0068e1e9d26ce08196b09bc0a157f0d351'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_0bc1f454cf4750db0068e1e9d26ce08196b09bc0a157f0d351'"}]}, "note": "ok"}
{"sample_index": 6, "subset": "indication", "disease": "Juvenile dermatomyositis", "relation": "indication", "gold_drugs": ["h.p. acthar gel"], "seeds": ["prednisone", "methotrexate", "intravenous immunoglobulin", "azathioprine", "mycophenolate mofetil", "hydroxychloroquine", "cyclosporine", "tacrolimus", "rituximab", "cyclophosphamide", "methylprednisolone", "etanercept"], "predictions": [], "ranking": ["prednisone", "methotrexate", "intravenous immunoglobulin", "azathioprine", "mycophenolate mofetil", "hydroxychloroquine", "cyclosporine", "tacrolimus", "rituximab", "cyclophosphamide", "methylprednisolone", "etanercept"], "top_recommendations": ["prednisone", "methotrexate", "intravenous immunoglobulin"], "run_dir": null, "config": {"config_tag": "O3-mini-single-SC", "flags": {"single_call": true, "n": 12, "model": "o3-mini", "self_consistency": 15}}, "debug_sc": {"trials": 15, "api_calls": 15, "pool_size": 37, "vote_table": [{"canonical": "prednisone", "votes": 13, "avg_rank": 1.0, "rep": "prednisone"}, {"canonical": "methotrexate", "votes": 13, "avg_rank": 2.3846, "rep": "methotrexate"}, {"canonical": "immunoglobulin", "votes": 13, "avg_rank": 4.0, "rep": "intravenous immunoglobulin"}, {"canonical": "azathioprine", "votes": 13, "avg_rank": 5.2308, "rep": "azathioprine"}, {"canonical": "mycophenolate mofetil", "votes": 13, "avg_rank": 6.0769, "rep": "mycophenolate mofetil"}, {"canonical": "hydroxychloroquine", "votes": 13, "avg_rank": 6.4615, "rep": "hydroxychloroquine"}, {"canonical": "cyclosporine", "votes": 13, "avg_rank": 7.3846, "rep": "cyclosporine"}, {"canonical": "tacrolimus", "votes": 13, "avg_rank": 9.0, "rep": "tacrolimus"}, {"canonical": "rituximab", "votes": 13, "avg_rank": 9.0769, "rep": "rituximab"}, {"canonical": "cyclophosphamide", "votes": 10, "avg_rank": 8.4, "rep": "cyclophosphamide"}, {"canonical": "methylprednisolone", "votes": 7, "avg_rank": 2.4286, "rep": "methylprednisolone"}, {"canonical": "etanercept", "votes": 6, "avg_rank": 12.0, "rep": "etanercept"}, {"canonical": "leflunomide", "votes": 4, "avg_rank": 11.25, "rep": "leflunomide"}, {"canonical": "infliximab", "votes": 3, "avg_rank": 11.3333, "rep": "infliximab"}, {"canonical": "{\"answers\" \"prednisone\"", "votes": 1, "avg_rank": 1.0, "rep": "{\"answers\": [\"prednisone\""}, {"canonical": "response id='resp 03d9f107a1f121610068e1e9ffcbc08197998c97b6e191b44c'", "votes": 1, "avg_rank": 1.0, "rep": "Response(id='resp_03d9f107a1f121610068e1e9ffcbc08197998c97b6e191b44c'"}, {"canonical": "\"methotrexate\"", "votes": 1, "avg_rank": 2.0, "rep": "\"methotrexate\""}, {"canonical": "created at=1759635967.0", "votes": 1, "avg_rank": 2.0, "rep": "created_at=1759635967.0"}, {"canonical": "error=none", "votes": 1, "avg_rank": 3.0, "rep": "error=None"}, {"canonical": "\"intravenous immunoglobulin\"", "votes": 1, "avg_rank": 3.0, "rep": "\"intravenous immunoglobulin\""}, {"canonical": "\"hydroxychloroquine\"", "votes": 1, "avg_rank": 4.0, "rep": "\"hydroxychloroquine\""}, {"canonical": "incomplete details=incompletedetails reason='max output tokens'", "votes": 1, "avg_rank": 4.0, "rep": "incomplete_details=IncompleteDetails(reason='max_output_tokens')"}, {"canonical": "instructions=none", "votes": 1, "avg_rank": 5.0, "rep": "instructions=None"}, {"canonical": "\"cyclophosphamide\"", "votes": 1, "avg_rank": 5.0, "rep": "\"cyclophosphamide\""}, {"canonical": "metadata={}", "votes": 1, "avg_rank": 6.0, "rep": "metadata={}"}, {"canonical": "\"azathioprine\"", "votes": 1, "avg_rank": 6.0, "rep": "\"azathioprine\""}, {"canonical": "\"mycophenolate mofetil\"", "votes": 1, "avg_rank": 7.0, "rep": "\"mycophenolate mofetil\""}, {"canonical": "model='o3 mini 2025 01 31'", "votes": 1, "avg_rank": 7.0, "rep": "model='o3-mini-2025-01-31'"}, {"canonical": "\"rituximab\"", "votes": 1, "avg_rank": 8.0, "rep": "\"rituximab\""}, {"canonical": "object='response'", "votes": 1, "avg_rank": 8.0, "rep": "object='response'"}, {"canonical": "\"cyclosporine\"", "votes": 1, "avg_rank": 9.0, "rep": "\"cyclosporine\""}, {"canonical": "output= responsereasoningitem id='rs 03d9f107a1f121610068e1ea0682d48197b2bb299b7aef4f79'", "votes": 1, "avg_rank": 9.0, "rep": "output=[ResponseReasoningItem(id='rs_03d9f107a1f121610068e1ea0682d48197b2bb299b7aef4f79'"}, {"canonical": "summary=", "votes": 1, "avg_rank": 10.0, "rep": "summary=[]"}, {"canonical": "chloroquine", "votes": 1, "avg_rank": 11.0, "rep": "chloroquine"}, {"canonical": "type='reasoning'", "votes": 1, "avg_rank": 11.0, "rep": "type='reasoning'"}, {"canonical": "tocilizumab", "votes": 1, "avg_rank": 12.0, "rep": "tocilizumab"}, {"canonical": "content=none", "votes": 1, "avg_rank": 12.0, "rep": "content=None"}]}, "note": "ok"}
